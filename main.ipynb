{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, BatchNormalization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "conditions = pd.read_csv('10k_synthea_covid19_csv/conditions.csv')\n",
    "immunizations = pd.read_csv('10k_synthea_covid19_csv/immunizations.csv')\n",
    "medications = pd.read_csv('10k_synthea_covid19_csv/medications.csv')\n",
    "observations = pd.read_csv('10k_synthea_covid19_csv/observations.csv')\n",
    "patients = pd.read_csv('10k_synthea_covid19_csv/patients.csv')\n",
    "allergies = pd.read_csv(r\"10k_synthea_covid19_csv/allergies.csv\")\n",
    "careplans = pd.read_csv('10k_synthea_covid19_csv/careplans.csv')\n",
    "\n",
    "# Ensure 'PATIENT' column exists in all datasets\n",
    "if 'Id' in patients.columns:\n",
    "    patients = patients.rename(columns={'Id': 'PATIENT'})\n",
    "\n",
    "# Check for PATIENT column consistency\n",
    "assert 'PATIENT' in patients.columns, \"PATIENT column missing in patients dataset\"\n",
    "assert 'PATIENT' in observations.columns, \"PATIENT column missing in observations dataset\"\n",
    "assert 'PATIENT' in conditions.columns, \"PATIENT column missing in conditions dataset\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Step 1: Consolidate all data into a single row per patient\n",
    "# One-hot encode categorical features for each dataset\n",
    "# Observations\n",
    "observations_onehot = pd.get_dummies(observations[['PATIENT', 'DESCRIPTION']], columns=['DESCRIPTION'], prefix='OBSERVATION')\n",
    "observations_features = observations_onehot.groupby('PATIENT').sum().reset_index()\n",
    "\n",
    "# Medications\n",
    "medications_onehot = pd.get_dummies(medications[['PATIENT', 'DESCRIPTION']], columns=['DESCRIPTION'], prefix='MEDICATION')\n",
    "medications_features = medications_onehot.groupby('PATIENT').sum().reset_index()\n",
    "\n",
    "# Care Plans\n",
    "careplans_onehot = pd.get_dummies(careplans[['PATIENT', 'DESCRIPTION']], columns=['DESCRIPTION'], prefix='CAREPLAN')\n",
    "careplans_features = careplans_onehot.groupby('PATIENT').sum().reset_index()\n",
    "\n",
    "# Immunizations\n",
    "immunizations_onehot = pd.get_dummies(immunizations[['PATIENT', 'DESCRIPTION']], columns=['DESCRIPTION'], prefix='IMMUNIZATION')\n",
    "immunizations_features = immunizations_onehot.groupby('PATIENT').sum().reset_index()\n",
    "\n",
    "# Conditions\n",
    "conditions_onehot = pd.get_dummies(conditions[['PATIENT', 'DESCRIPTION']], columns=['DESCRIPTION'], prefix='CONDITION')\n",
    "conditions_features = conditions_onehot.groupby('PATIENT').sum().reset_index()\n",
    "\n",
    "# Allergies\n",
    "allergies_onehot = pd.get_dummies(allergies[['PATIENT', 'DESCRIPTION']], columns=['DESCRIPTION'], prefix='ALLERGY')\n",
    "allergies_features = allergies_onehot.groupby('PATIENT').sum().reset_index()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Categorical columns being encoded: Index(['RACE', 'ETHNICITY', 'GENDER', 'BIRTHPLACE', 'COUNTY'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "# Step 2: Merge all features into a single DataFrame\n",
    "patients = patients.rename(columns={'Id': 'PATIENT'})  # Rename Id to PATIENT for consistency\n",
    "merged_features = patients.copy()\n",
    "merged_features = merged_features.merge(observations_features, on='PATIENT', how='left')\n",
    "merged_features = merged_features.merge(medications_features, on='PATIENT', how='left')\n",
    "merged_features = merged_features.merge(careplans_features, on='PATIENT', how='left')\n",
    "merged_features = merged_features.merge(immunizations_features, on='PATIENT', how='left')\n",
    "merged_features = merged_features.merge(conditions_features, on='PATIENT', how='left')\n",
    "merged_features = merged_features.merge(allergies_features, on='PATIENT', how='left')\n",
    "\n",
    "# Fill missing values\n",
    "merged_features.fillna(0, inplace=True)\n",
    "\n",
    "# Convert date fields to meaningful numeric features\n",
    "merged_features['BIRTHDATE'] = pd.to_datetime(merged_features['BIRTHDATE'], errors='coerce')\n",
    "merged_features['DEATHDATE'] = pd.to_datetime(merged_features['DEATHDATE'], errors='coerce')\n",
    "merged_features['AGE'] = (pd.Timestamp.now().year - merged_features['BIRTHDATE'].dt.year).fillna(0).astype(int)\n",
    "merged_features['AGE_AT_DEATH'] = (merged_features['DEATHDATE'].dt.year - merged_features['BIRTHDATE'].dt.year).fillna(0).astype(int)\n",
    "\n",
    "# Remove unnecessary columns\n",
    "irrelevant_columns = ['SSN', 'DRIVERS', 'PASSPORT', 'PREFIX', 'FIRST', 'LAST', 'SUFFIX', 'MAIDEN', 'MARITAL', 'ADDRESS', 'CITY', 'STATE', 'ZIP']\n",
    "merged_features.drop(columns=irrelevant_columns, inplace=True, errors='ignore')\n",
    "\n",
    "# One-hot encode remaining categorical columns, including RACE, GENDER, and ETHNICITY, but exclude 'PATIENT'\n",
    "categorical_columns = merged_features.select_dtypes(include=['object', 'category']).columns\n",
    "categorical_columns = categorical_columns[categorical_columns != 'PATIENT']  # Exclude 'PATIENT' column\n",
    "print(f\"Categorical columns being encoded: {categorical_columns}\")  # Debugging step\n",
    "\n",
    "merged_features = pd.get_dummies(merged_features, columns=categorical_columns, drop_first=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Step 3: Add target variable\n",
    "# Determine COVID-19 status\n",
    "covid_code = \"840539006\"  # Example ICD-10 code for COVID-19\n",
    "covid_patients = conditions[conditions['CODE'] == covid_code]['PATIENT'].unique()\n",
    "assert 'PATIENT' in merged_features.columns, \"PATIENT column missing in merged_features\"\n",
    "merged_features['COVID'] = merged_features['PATIENT'].isin(covid_patients).astype(int)\n",
    "\n",
    "# Determine death status\n",
    "merged_features['DIED'] = merged_features['DEATHDATE'].notna().astype(int)\n",
    "\n",
    "# Classify patients into four categories\n",
    "merged_features['CLASS'] = merged_features.apply(\n",
    "    lambda row: (\n",
    "        'had_covid_died' if row['COVID'] and row['DIED'] else\n",
    "        'had_covid_lived' if row['COVID'] and not row['DIED'] else\n",
    "        'no_covid_died' if not row['COVID'] and row['DIED'] else\n",
    "        'no_covid_lived'\n",
    "    ),\n",
    "    axis=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Step 4: Prepare dataset for training\n",
    "X = merged_features.drop(columns=['PATIENT', 'COVID', 'DIED', 'BIRTHDATE', 'DEATHDATE', 'CLASS'])\n",
    "y = merged_features['CLASS']\n",
    "\n",
    "# Encode target labels\n",
    "label_encoder = LabelEncoder()\n",
    "y_encoded = label_encoder.fit_transform(y)\n",
    "\n",
    "# Standardize numerical features\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Split into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y_encoded, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "247/247 [==============================] - 6s 13ms/step - loss: 0.0000e+00 - accuracy: 0.0000e+00 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00\n",
      "Epoch 2/20\n",
      "247/247 [==============================] - 3s 12ms/step - loss: 0.0000e+00 - accuracy: 0.0000e+00 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00\n",
      "Epoch 3/20\n",
      "247/247 [==============================] - 3s 12ms/step - loss: 0.0000e+00 - accuracy: 0.0000e+00 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00\n",
      "Epoch 4/20\n",
      "247/247 [==============================] - 3s 12ms/step - loss: 0.0000e+00 - accuracy: 0.0000e+00 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00\n",
      "Epoch 5/20\n",
      "247/247 [==============================] - 3s 12ms/step - loss: 0.0000e+00 - accuracy: 0.0000e+00 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00\n",
      "Epoch 6/20\n",
      "247/247 [==============================] - 3s 12ms/step - loss: 0.0000e+00 - accuracy: 0.0000e+00 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00\n",
      "Epoch 7/20\n",
      "247/247 [==============================] - 3s 12ms/step - loss: 0.0000e+00 - accuracy: 0.0000e+00 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00\n",
      "Epoch 8/20\n",
      "247/247 [==============================] - 3s 12ms/step - loss: 0.0000e+00 - accuracy: 0.0000e+00 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00\n",
      "Epoch 9/20\n",
      "247/247 [==============================] - 3s 12ms/step - loss: 0.0000e+00 - accuracy: 0.0000e+00 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00\n",
      "Epoch 10/20\n",
      "247/247 [==============================] - 3s 12ms/step - loss: 0.0000e+00 - accuracy: 0.0000e+00 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00\n",
      "Epoch 11/20\n",
      "247/247 [==============================] - 3s 12ms/step - loss: 0.0000e+00 - accuracy: 0.0000e+00 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00\n",
      "Epoch 12/20\n",
      "247/247 [==============================] - 3s 12ms/step - loss: 0.0000e+00 - accuracy: 0.0000e+00 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00\n",
      "Epoch 13/20\n",
      "247/247 [==============================] - 3s 12ms/step - loss: 0.0000e+00 - accuracy: 0.0000e+00 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00\n",
      "Epoch 14/20\n",
      "247/247 [==============================] - 3s 12ms/step - loss: 0.0000e+00 - accuracy: 0.0000e+00 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00\n",
      "Epoch 15/20\n",
      "247/247 [==============================] - 3s 12ms/step - loss: 0.0000e+00 - accuracy: 0.0000e+00 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00\n",
      "Epoch 16/20\n",
      "247/247 [==============================] - 3s 12ms/step - loss: 0.0000e+00 - accuracy: 0.0000e+00 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00\n",
      "Epoch 17/20\n",
      "247/247 [==============================] - 3s 11ms/step - loss: 0.0000e+00 - accuracy: 0.0000e+00 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00\n",
      "Epoch 18/20\n",
      "247/247 [==============================] - 3s 12ms/step - loss: 0.0000e+00 - accuracy: 0.0000e+00 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00\n",
      "Epoch 19/20\n",
      "247/247 [==============================] - 3s 12ms/step - loss: 0.0000e+00 - accuracy: 0.0000e+00 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00\n",
      "Epoch 20/20\n",
      "247/247 [==============================] - 3s 11ms/step - loss: 0.0000e+00 - accuracy: 0.0000e+00 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00\n",
      "Test Loss: 0.0\n",
      "Test Accuracy: 0.0\n",
      "Model saved as 'covid_mortality_full_model.h5'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\sagor\\anaconda3\\lib\\site-packages\\keras\\src\\engine\\training.py:3000: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Step 5: Define CNN model\n",
    "model = Sequential([\n",
    "    Dense(512, activation='relu', input_shape=(X_train.shape[1],)),\n",
    "    BatchNormalization(),\n",
    "    Dropout(0.5),\n",
    "    Dense(256, activation='relu'),\n",
    "    BatchNormalization(),\n",
    "    Dropout(0.4),\n",
    "    Dense(128, activation='relu'),\n",
    "    BatchNormalization(),\n",
    "    Dropout(0.3),\n",
    "    Dense(len(label_encoder.classes_), activation='softmax')\n",
    "])\n",
    "\n",
    "# Compile model\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train model\n",
    "history = model.fit(X_train, y_train, validation_split=0.2, epochs=20, batch_size=32, verbose=1)\n",
    "\n",
    "# Evaluate model\n",
    "test_loss, test_accuracy = model.evaluate(X_test, y_test, verbose=0)\n",
    "print(f\"Test Loss: {test_loss}\")\n",
    "print(f\"Test Accuracy: {test_accuracy}\")\n",
    "\n",
    "# Save model\n",
    "model.save('covid_mortality_full_model.h5')\n",
    "print(\"Model saved as 'covid_mortality_full_model.h5'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
